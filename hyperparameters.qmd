---
title: "Lab 8: Hyperparameters"
author: "Brandon Pizer ESS-330"
format:
  html:
    self-contained: true
---
**Initial Packages**
```{r}
library(tidyverse)
library(tidymodels)
library(powerjoin)
library(readr)
library(skimr)
library(visdat)
library(ggpubr)
library(patchwork)
library(glue)
library(baguette)
```

**Data Retrieval**
```{r}
root  <- 'https://gdex.ucar.edu/dataset/camels/file'

types <- c("clim", "geol", "soil", "topo", "vege", "hydro")


remote_files  <- glue('{root}/camels_{types}.txt')
local_files   <- glue('data/camels_{types}.txt')

walk2(remote_files, local_files, download.file, quiet = TRUE)

camels <- map(local_files, read_delim, show_col_types = FALSE) 

camels <- power_full_join(camels ,by = 'gauge_id')
```

**Data Cleaning**
```{r}

vis_dat(camels)
vis_miss(camels, cluster = TRUE)

```
```{r}
camels_clean <- camels %>%
  drop_na()


names(camels_clean)

```

**Data Splitting**
```{r}
set.seed(10262004)

camels_split <- initial_split(camels_clean, prop = 0.8)
camels_train <- training(camels_split)
camels_test  <- testing(camels_split)

```

**Recipe**
```{r}

library(recipes)

library(recipes)

camels_rec <- recipe(q_mean ~ ., data = camels_train) %>%
  update_role(gauge_lat, gauge_lon, new_role = "ID") %>% 
  step_rm(gauge_id) %>% 
  step_dummy(all_nominal(), -all_outcomes()) %>%  
  step_impute_mean(all_numeric_predictors()) 

#I was having a ton of errors so this was the fix that chatgpt gave me


```

**Resampling**
```{r}

set.seed(10262004) 

camels_cv <- vfold_cv(camels_train, v = 10)

```

**Defining Models**
```{r}

nn_model <- bag_mlp() %>%
  set_engine("nnet") %>%
  set_mode("regression")

rf_model <- rand_forest() %>%
  set_engine("randomForest") %>%
  set_mode("regression")

xgb_model <- boost_tree() %>%
  set_engine("xgboost") %>%
  set_mode("regression")



```


```{r}

wf_set <- workflow_set(
  list(camels_rec),  
  list(nn_model, rf_model, xgb_model),  
  cross = TRUE  
)

wf_results <- wf_set %>%
  workflow_map(
    "fit_resamples", 
    resamples = camels_cv,
    control = control_grid(save_pred = TRUE)
  )

autoplot(wf_results)


```
Out of the metrics, I am going to select XGBoost because it has a very high rsq and the lowest rmse. The model type is a boosted tree. The engine is xgboost. The mode is regression. I think its a good fit because the XGBoost model is robust, as it performs well with complex relationships present in the camels dataset.

**Model Testing**
```{r}

xgb_model <- boost_tree(
  trees = tune(),           
  tree_depth = tune()       
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

xgb_workflow <- workflow() %>%
  add_recipe(camels_rec) %>%  
  add_model(xgb_model)  

xgb_grid <- grid_regular(
  trees(),
  tree_depth(),
  levels = 5
)


xgb_cv_resamples <- vfold_cv(camels_train, v = 10)


xgb_tune_results <- tune_grid(
  xgb_workflow,          
  resamples = xgb_cv_resamples,
  grid = xgb_grid,
  metrics = metric_set(rmse, rsq)  
)


xgb_tune_results


```
**Hyperparameter Tuning**
```{r}

xgb_tune_results <- tune_grid(
  xgb_workflow,          
  resamples = xgb_cv_resamples,
  grid = xgb_grid,
  metrics = metric_set(rmse, rsq, mae)  
)


autoplot(xgb_tune_results)

```
```{r}

tuned_metrics <- collect_metrics(xgb_tune_results)

head(tuned_metrics)
show_best(xgb_tune_results)

```

```{r}

best_model_mae <- show_best(xgb_tune_results, metric = "mae")

best_model_mae


```
This is the combination of hyperparameters that gave the best value for mean absolute error. 
**Hyperparameter Model Selection**
```{r}

hp_best <- select_best(xgb_tune_results, metric = "mae")


hp_best


```
```{r}

xgb_final_wf <- finalize_workflow(
  xgb_workflow,
  select_best(xgb_tune_results, metric = "mae")
)

xgb_final_wf

```
```{r}
xgb_final_fit <- last_fit(
  xgb_final_wf,
  split = camels_split
)

collect_metrics(xgb_final_fit)

```
**Graph**
```{r}
xgb_final_predictions <- collect_predictions(xgb_final_fit)


xgb_final_fit %>%
  collect_predictions() %>%
  ggplot(aes(x = .pred, y = q_mean)) +
  geom_point(alpha = 0.6, color = "#0073C2FF") +
  geom_smooth(method = "lm", se = FALSE, color = "darkred") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gray40") +
  labs(
    title = "Predicted vs Actual Mean Daily Discharge using Baseflow Index",
    x = "Predicted Mean Daily Discharge",
    y = "Actual Mean Daily Discharge"
  ) +
  theme_minimal()


```
**Mapping**
```{r}

final_fit_all <- fit(xgb_final_wf, data = camels_clean)



full_preds <- augment(final_fit_all, new_data = camels_clean)

full_preds <- full_preds %>%
  mutate(residuals = (q_mean - .pred)^2)



map_pred <- ggplot(full_preds, aes(x = gauge_lon, y = gauge_lat, color = .pred)) +
  geom_point(size = 2) +
  scale_color_viridis_c(option = "plasma") +
  coord_fixed(1.3) +
  labs(
    title = "Predicted q_mean Across CONUS",
    color = "Prediction"
  ) +
  theme_minimal()

map_resid <- ggplot(full_preds, aes(x = gauge_lon, y = gauge_lat, color = residuals)) +
  geom_point(size = 2) +
  scale_color_viridis_c(option = "inferno") +
  coord_fixed(1.3) +
  labs(
    title = "Residuals (Squared Error) Across CONUS",
    color = "Residuals"
  ) +
  theme_minimal()


```

```{r}
library(patchwork)

map_pred + map_resid +
  plot_annotation(title = "Model Predictions and Residuals Across the US")

```





